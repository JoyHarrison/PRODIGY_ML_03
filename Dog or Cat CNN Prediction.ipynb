{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7681529-7624-47b0-a467-c597f18c7cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50a993ae-2be1-4e4d-bd42-0695ac442f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.14.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a176de2e-f1bc-4ae9-b9c2-c5e8f5104b5f",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7addbe-12e6-4ca4-8354-d1e12ccee904",
   "metadata": {},
   "source": [
    "Image augmentation to prevent Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf5c773-6cfa-405a-a42b-163894c6ce84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Create a tkinter window for the file dialog\n",
    "window = tk.Tk()\n",
    "window.withdraw()  # Hide the window\n",
    "\n",
    "# Open a file dialog to let the user select the directory\n",
    "user_path = filedialog.askdirectory()\n",
    "\n",
    "# Close the tkinter window\n",
    "window.destroy()\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   shear_range=0.2,\n",
    "                                   zoom_range=0.2,\n",
    "                                   horizontal_flip=True)\n",
    "\n",
    "# Use the user-selected path\n",
    "training_set = train_datagen.flow_from_directory(user_path,\n",
    "                                                 target_size=(64, 64),\n",
    "                                                 batch_size=32,\n",
    "                                                 class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac58f47c-3635-4877-aa41-fb3132ab27c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('D:\\JOY\\J AMITY\\Machine Learning\\CNN model pics\\CNN udem\\dataset/test_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30f5918-5bd8-49fe-8302-35785ae00f2c",
   "metadata": {},
   "source": [
    "# Building CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e261de4-407a-4bf6-86d6-deb67f9a5a2e",
   "metadata": {},
   "source": [
    "Initializing CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f327e6b9-f505-4f9c-bb63-898561fc4fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn=tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdaba7db-758b-4fde-bcd4-f45ddfe7765e",
   "metadata": {},
   "source": [
    "Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ab1932c-a8ac-41aa-959f-a4538e49b692",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu',input_shape=[64,64,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a28db6a-4a84-4467-94bc-d7f43f6cb55f",
   "metadata": {},
   "source": [
    "Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9525025-c9e8-4899-91df-669c59325696",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492b5cd4-e7e0-4b3c-a2da-b8acdba0cf11",
   "metadata": {},
   "source": [
    "2nd Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5035113-7801-4286-a540-ca8bf31a0402",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32,kernel_size=3,activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78b0a17-f006-48a2-9c5d-9e487f849a6d",
   "metadata": {},
   "source": [
    "Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6da37e9d-9f0a-4d56-bd88-44b60d99f727",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80813b0b-d20a-4845-b59c-cabc05585847",
   "metadata": {},
   "source": [
    "Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7105de70-61f7-4793-8572-3b8a68885898",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6b3f1c-fada-4ed0-9a2c-f06afdd4ea48",
   "metadata": {},
   "source": [
    "Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "329ea84f-e38c-42fe-a17e-69d96d49b4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737a8a8e-82ac-4453-8b9f-d4b22420f5f3",
   "metadata": {},
   "source": [
    "# Training CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7d62cc-8afb-4cf1-a263-75add809dfdc",
   "metadata": {},
   "source": [
    "Compiling CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "365b611b-c7fc-4205-be4a-7a7c4c9342bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc00e86-f2a1-4703-ae81-81d51c86b4cd",
   "metadata": {},
   "source": [
    "Training CNN on training set and evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b08a027-8e71-4b87-b732-4e2d0119e882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "63/63 [==============================] - 27s 413ms/step - loss: 0.6997 - accuracy: 0.5310 - val_loss: 0.6691 - val_accuracy: 0.6200\n",
      "Epoch 2/25\n",
      "63/63 [==============================] - 15s 236ms/step - loss: 0.6681 - accuracy: 0.6090 - val_loss: 0.6367 - val_accuracy: 0.6490\n",
      "Epoch 3/25\n",
      "63/63 [==============================] - 14s 227ms/step - loss: 0.6418 - accuracy: 0.6190 - val_loss: 0.6170 - val_accuracy: 0.6605\n",
      "Epoch 4/25\n",
      "63/63 [==============================] - 15s 233ms/step - loss: 0.6414 - accuracy: 0.6255 - val_loss: 0.6132 - val_accuracy: 0.6485\n",
      "Epoch 5/25\n",
      "63/63 [==============================] - 14s 221ms/step - loss: 0.6199 - accuracy: 0.6650 - val_loss: 0.5826 - val_accuracy: 0.7005\n",
      "Epoch 6/25\n",
      "63/63 [==============================] - 13s 209ms/step - loss: 0.6229 - accuracy: 0.6655 - val_loss: 0.5986 - val_accuracy: 0.6595\n",
      "Epoch 7/25\n",
      "63/63 [==============================] - 14s 223ms/step - loss: 0.6003 - accuracy: 0.6755 - val_loss: 0.5701 - val_accuracy: 0.6950\n",
      "Epoch 8/25\n",
      "63/63 [==============================] - 14s 225ms/step - loss: 0.5754 - accuracy: 0.6980 - val_loss: 0.5373 - val_accuracy: 0.7310\n",
      "Epoch 9/25\n",
      "63/63 [==============================] - 15s 237ms/step - loss: 0.5625 - accuracy: 0.7105 - val_loss: 0.5081 - val_accuracy: 0.7510\n",
      "Epoch 10/25\n",
      "63/63 [==============================] - 15s 233ms/step - loss: 0.5352 - accuracy: 0.7325 - val_loss: 0.4959 - val_accuracy: 0.7565\n",
      "Epoch 11/25\n",
      "63/63 [==============================] - 14s 217ms/step - loss: 0.5251 - accuracy: 0.7390 - val_loss: 0.4920 - val_accuracy: 0.7560\n",
      "Epoch 12/25\n",
      "63/63 [==============================] - 14s 222ms/step - loss: 0.5047 - accuracy: 0.7625 - val_loss: 0.4348 - val_accuracy: 0.7900\n",
      "Epoch 13/25\n",
      "63/63 [==============================] - 15s 238ms/step - loss: 0.4786 - accuracy: 0.7625 - val_loss: 0.4001 - val_accuracy: 0.8195\n",
      "Epoch 14/25\n",
      "63/63 [==============================] - 15s 240ms/step - loss: 0.4397 - accuracy: 0.7865 - val_loss: 0.3963 - val_accuracy: 0.8110\n",
      "Epoch 15/25\n",
      "63/63 [==============================] - 15s 232ms/step - loss: 0.4460 - accuracy: 0.7880 - val_loss: 0.3644 - val_accuracy: 0.8345\n",
      "Epoch 16/25\n",
      "63/63 [==============================] - 15s 237ms/step - loss: 0.4057 - accuracy: 0.8175 - val_loss: 0.3968 - val_accuracy: 0.8105\n",
      "Epoch 17/25\n",
      "63/63 [==============================] - 14s 226ms/step - loss: 0.4121 - accuracy: 0.8025 - val_loss: 0.3300 - val_accuracy: 0.8530\n",
      "Epoch 18/25\n",
      "63/63 [==============================] - 15s 233ms/step - loss: 0.3777 - accuracy: 0.8215 - val_loss: 0.2891 - val_accuracy: 0.8770\n",
      "Epoch 19/25\n",
      "63/63 [==============================] - 14s 228ms/step - loss: 0.3806 - accuracy: 0.8235 - val_loss: 0.3099 - val_accuracy: 0.8710\n",
      "Epoch 20/25\n",
      "63/63 [==============================] - 15s 232ms/step - loss: 0.3455 - accuracy: 0.8475 - val_loss: 0.2552 - val_accuracy: 0.8990\n",
      "Epoch 21/25\n",
      "63/63 [==============================] - 15s 244ms/step - loss: 0.3237 - accuracy: 0.8460 - val_loss: 0.2480 - val_accuracy: 0.8925\n",
      "Epoch 22/25\n",
      "63/63 [==============================] - 14s 227ms/step - loss: 0.3108 - accuracy: 0.8670 - val_loss: 0.2124 - val_accuracy: 0.9095\n",
      "Epoch 23/25\n",
      "63/63 [==============================] - 14s 229ms/step - loss: 0.2849 - accuracy: 0.8800 - val_loss: 0.2721 - val_accuracy: 0.8765\n",
      "Epoch 24/25\n",
      "63/63 [==============================] - 14s 226ms/step - loss: 0.2552 - accuracy: 0.8945 - val_loss: 0.1818 - val_accuracy: 0.9270\n",
      "Epoch 25/25\n",
      "63/63 [==============================] - 14s 227ms/step - loss: 0.2751 - accuracy: 0.8830 - val_loss: 0.2329 - val_accuracy: 0.8985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x228304cd310>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.fit(x=training_set,validation_data=test_set,epochs=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b0b9c5-8e3c-4cb0-accd-e607f72af344",
   "metadata": {},
   "source": [
    "Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0282825-6f73-4444-994e-95e3cdf7b946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "test_image = image.load_img('D:\\JOY\\J AMITY\\Machine Learning\\CNN model pics\\CNN udem\\dataset/single_prediction/cat_or_dog_1.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87dc0787-d5e4-4ce1-99e0-310b3a3cdc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "test_image = image.load_img('D:\\JOY\\J AMITY\\Machine L9earning\\CNN model pics\\CNN udem\\dataset/single_prediction/cat_or_dog_2.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "  prediction = 'dog'\n",
    "else:\n",
    "  prediction = 'cat'\n",
    "\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1cfd86-6ce0-4813-8088-7a29e64af123",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
